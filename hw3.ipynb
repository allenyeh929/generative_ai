{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqHdj2gdZgtEAPhgD8Qoea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allenyeh929/generative_ai/blob/main/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross Entropy (交叉熵)\n",
        "* 對於真實分佈 $p(x)$ 與預測分佈 $q(x)$，交叉熵定義為：\n",
        "$$\n",
        "H(p, q) = - \\sum_{x} p(x) \\log q(x)\n",
        "$$\n",
        "  * 它度量了如果以 $q$ 為基準時，描述 $p$ 需要的額外信息量。\n",
        "\n",
        "#### KL Divergence (相對熵)\n",
        "* 定義：\n",
        "$$\n",
        "D_{KL}(p \\parallel q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)}\n",
        "$$\n",
        "  * 它衡量了使用 $q$ 去近似 $p$ 所造成的信息損失，或稱為“分佈之間的差異”。需要注意的是，KL散度不是一個對稱度量，即 $D_{KL}(p \\parallel q) \\neq D_{KL}(q \\parallel p)$ 。\n",
        "\n",
        "#### Cross Entropy 及 KL Divergence 的關係\n",
        "* 交叉熵可以拆分成兩部分：真實分佈 $p$ 的 entropy 與 $p$ 與 $q$ 之間的 KL Divergence，即：\n",
        "$$\n",
        "H(p, q) = H(p) + D_{KL}(p \\parallel q)\n",
        "$$\n",
        "  * entropy 的定義: $H(p) = - \\sum_{x} p(x) \\log p(x)$\n",
        "* 當 $p$ 固定時，最小化交叉熵等同於最小化相對熵，這也是為什麼在很多分類問題中直接使用交叉熵作為損失函數的原因。\n",
        "\n",
        "#### 使用時機\n",
        "* Cross Entropy 的使用時機\n",
        "  * 機器學習中的損失函數：在分類問題中（例如神經網絡、邏輯回歸等），交叉熵常作為損失函數。因為在這些任務中，真實標籤可以看作是離散分佈（通常是one-hot向量），而模型輸出則代表預測分佈。\n",
        "  * 模型訓練：在訓練過程中，最小化交叉熵可以促使預測分佈 $q$ 趨近於真實分佈 $p$ 。\n",
        "* KL Divergence 的使用時機\n",
        "  * 衡量分佈差異：KL Divergence 常用於評估兩個概率分佈之間的差異或信息損失，無論是在統計學、信息論還是機器學習中。\n",
        "  * 生成模型與變分推斷：在變分自編碼器（Variational Autoencoder, VAE）等生成模型中，KL Divergence 用來衡量近似後驗分佈與真實後驗分佈的差距，進而引導模型學習更好的近似。\n",
        "  * 分佈匹配問題：例如在概率圖模型或貝葉斯推斷中，KL Divergence 可以用來衡量模型預測與觀察數據之間的差異。"
      ],
      "metadata": {
        "id": "-HIu1mms9w-e"
      }
    }
  ]
}